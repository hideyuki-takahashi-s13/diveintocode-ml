{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】（アドバンス課題）様々な手法を実行\n",
    "TensorFLowやGoogle AI ResearchのGitHubリポジトリには、定番のモデルから最新のモデルまで多様なコードが公開されています。これらから興味あるものを選び実行してください。\n",
    "\n",
    "\n",
    "なお、これらのコードは初学者向けではないため、巨大なデータセットのダウンロードが必要な場合など、実行が簡単ではないこともあります。そういった場合は、コードリーディングを行ってください。\n",
    "\n",
    "\n",
    "models/research at master · tensorflow/models\n",
    "\n",
    "\n",
    "google-research/google-research: Google AI Research\n",
    "\n",
    "\n",
    "更新日が古いものはPythonやTensorFlowのバージョンが古く、扱いずらい場合があります。新しいものから見ることを推奨します。\n",
    "\n",
    "## 回答無し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】Iris（2値分類）をKerasで学習\n",
    "TensorFlowによるIrisデータセットに対する2値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/takahashihideyuki/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/takahashihideyuki/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/takahashihideyuki/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/takahashihideyuki/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/takahashihideyuki/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/takahashihideyuki/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/takahashihideyuki/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/takahashihideyuki/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/takahashihideyuki/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/takahashihideyuki/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/takahashihideyuki/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/takahashihideyuki/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/takahashihideyuki/dive/diveintocode-ml/Sprint_/datasets_19_420_Iris.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TensorFlowで実装したIris2値分類をKerasに書き換える\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "# データセットの読み込み\n",
    "dataset_path =\"/Users/takahashihideyuki/dive/diveintocode-ml/Sprint_/datasets_19_420_Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_1/kernel:0' shape=(4, 50) dtype=float32>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(50,) dtype=float32>,\n",
       " <tf.Variable 'dense_2/kernel:0' shape=(50, 100) dtype=float32>,\n",
       " <tf.Variable 'dense_2/bias:0' shape=(100,) dtype=float32>,\n",
       " <tf.Variable 'dense_3/kernel:0' shape=(100, 1) dtype=float32>,\n",
       " <tf.Variable 'dense_3/bias:0' shape=(1,) dtype=float32>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_dim=4))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/takahashihideyuki/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/takahashihideyuki/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/takahashihideyuki/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/takahashihideyuki/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "Train on 64 samples, validate on 16 samples\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.9911 - accuracy: 0.3750 - val_loss: 0.9101 - val_accuracy: 0.3750\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 0s 165us/step - loss: 0.6643 - accuracy: 0.6719 - val_loss: 0.6399 - val_accuracy: 0.6250\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 0s 145us/step - loss: 0.6839 - accuracy: 0.5312 - val_loss: 0.6461 - val_accuracy: 0.6250\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 0s 152us/step - loss: 0.5951 - accuracy: 0.6875 - val_loss: 0.6433 - val_accuracy: 0.3750\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 0s 146us/step - loss: 0.5594 - accuracy: 0.6406 - val_loss: 0.5519 - val_accuracy: 0.6875\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 0s 149us/step - loss: 0.6899 - accuracy: 0.5469 - val_loss: 0.7269 - val_accuracy: 0.3750\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 0s 148us/step - loss: 0.6445 - accuracy: 0.5938 - val_loss: 0.4849 - val_accuracy: 0.9375\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 0s 149us/step - loss: 0.5380 - accuracy: 0.6875 - val_loss: 0.4708 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 0s 141us/step - loss: 0.4706 - accuracy: 0.7969 - val_loss: 0.4161 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 0s 139us/step - loss: 0.4176 - accuracy: 0.8594 - val_loss: 0.3840 - val_accuracy: 0.9375\n",
      "test_acc : 0.800\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, \n",
    "                epochs=10, \n",
    "                batch_size=10, \n",
    "                verbose=1,\n",
    "                validation_data=(X_val, y_val))\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred_proba [0.8340291  0.2873689  0.7985698  0.26576668 0.7739522  0.43942082\n",
      " 0.22608674 0.4462025  0.6705725  0.81229687 0.25382882 0.7392523\n",
      " 0.50927544 0.7262279  0.62055963 0.3844298  0.6839589  0.74016714\n",
      " 0.41273665 0.7474428  0.88030285 0.46800742 0.7114785  0.88175726\n",
      " 0.79767644 0.3036275  0.77518606 0.24171063 0.70308304 0.82700825\n",
      " 0.8731663  0.83523214 0.527051   0.7227859  0.37227023 0.91407424\n",
      " 0.833507   0.23430184 0.30126023 0.52139217 0.16636312 0.70157206\n",
      " 0.4267404  0.15014073 0.36934036 0.49878806 0.8630189  0.8915274\n",
      " 0.23039603 0.5372529  0.25802708 0.8097034  0.8366812  0.8402703\n",
      " 0.8580705  0.795363   0.41755486 0.76651835 0.63791484 0.875298\n",
      " 0.19785145 0.4034803  0.78607786 0.623826  ]\n",
      "y_pred [1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1\n",
      " 0 0 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_train)[:, 0]\n",
    "# 確率を0, 1に変換\n",
    "y_pred = np.where(y_pred_proba >0.5, 1, 0)\n",
    "print(\"y_pred_proba\", y_pred_proba)\n",
    "print(\"y_pred\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】Iris（多値分類）をKerasで学習\n",
    "TensorFlowによるIrisデータセットに対する3値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TensorFlowで実装したIris多値分類をKerasに書き換える\n",
    "\"\"\"\n",
    "# データセットの読み込み\n",
    "dataset_path =\"/Users/takahashihideyuki/dive/diveintocode-ml/Sprint_/datasets_19_420_Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-setosa'] = 0 #Iris-setosaを追加\n",
    "y[y=='Iris-versicolor'] = 1\n",
    "y[y=='Iris-virginica'] = 2\n",
    "y = y.astype(np.int) #[:, np.newaxis]\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y = enc.fit_transform(y[:, np.newaxis]) #one-hot処理\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_1/kernel:0' shape=(4, 50) dtype=float32>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(50,) dtype=float32>,\n",
       " <tf.Variable 'dense_2/kernel:0' shape=(50, 100) dtype=float32>,\n",
       " <tf.Variable 'dense_2/bias:0' shape=(100,) dtype=float32>,\n",
       " <tf.Variable 'dense_3/kernel:0' shape=(100, 3) dtype=float32>,\n",
       " <tf.Variable 'dense_3/bias:0' shape=(3,) dtype=float32>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_dim=4))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96 samples, validate on 24 samples\n",
      "Epoch 1/10\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.9225 - accuracy: 0.6354 - val_loss: 0.4819 - val_accuracy: 0.6250\n",
      "Epoch 2/10\n",
      "96/96 [==============================] - 0s 129us/step - loss: 0.4023 - accuracy: 0.8229 - val_loss: 0.3761 - val_accuracy: 0.7083\n",
      "Epoch 3/10\n",
      "96/96 [==============================] - 0s 118us/step - loss: 0.2503 - accuracy: 0.9583 - val_loss: 0.2851 - val_accuracy: 0.9167\n",
      "Epoch 4/10\n",
      "96/96 [==============================] - 0s 127us/step - loss: 0.2217 - accuracy: 0.8750 - val_loss: 0.2184 - val_accuracy: 0.9167\n",
      "Epoch 5/10\n",
      "96/96 [==============================] - 0s 118us/step - loss: 0.2156 - accuracy: 0.8958 - val_loss: 0.5576 - val_accuracy: 0.7083\n",
      "Epoch 6/10\n",
      "96/96 [==============================] - 0s 121us/step - loss: 0.1446 - accuracy: 0.9271 - val_loss: 0.1875 - val_accuracy: 0.9167\n",
      "Epoch 7/10\n",
      "96/96 [==============================] - 0s 121us/step - loss: 0.0874 - accuracy: 0.9688 - val_loss: 0.2080 - val_accuracy: 0.9167\n",
      "Epoch 8/10\n",
      "96/96 [==============================] - 0s 119us/step - loss: 0.0878 - accuracy: 0.9688 - val_loss: 0.2156 - val_accuracy: 0.9167\n",
      "Epoch 9/10\n",
      "96/96 [==============================] - 0s 116us/step - loss: 0.0931 - accuracy: 0.9583 - val_loss: 0.2890 - val_accuracy: 0.9167\n",
      "Epoch 10/10\n",
      "96/96 [==============================] - 0s 117us/step - loss: 0.0755 - accuracy: 0.9583 - val_loss: 0.1800 - val_accuracy: 0.9167\n",
      "test_acc : 1.000\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, \n",
    "                epochs=10, \n",
    "                batch_size=10, \n",
    "                verbose=1,\n",
    "                validation_data=(X_val, y_val))\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題5】House PricesをKerasで学習\n",
    "TensorFlowによるHouse Pricesデータセットに対する回帰をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path =\"/Users/takahashihideyuki/dive/diveintocode-ml/Sprint_/train.csv\"\n",
    "df = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TensorFlowで実装した回帰問題のデータセットであるHouse Pricesを使用したモデルをKerasに書き換える\n",
    "\"\"\"\n",
    "# データセットの読み込み\n",
    "dataset_path =\"/Users/takahashihideyuki/dive/diveintocode-ml/Sprint_/train.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "y = df[\"SalePrice\"]\n",
    "X = df[[\"GrLivArea\", \"YearBuilt\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "\n",
    "# 目的変数を対数変換\n",
    "y = np.log1p(y).reshape(-1, 1)\n",
    "\n",
    "#説明変数を標準化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_dim=2))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='Adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                150       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 5,351\n",
      "Trainable params: 5,351\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 934 samples, validate on 234 samples\n",
      "Epoch 1/10\n",
      "934/934 [==============================] - 0s 209us/step - loss: 75.4729 - val_loss: 9.4904\n",
      "Epoch 2/10\n",
      "934/934 [==============================] - 0s 80us/step - loss: 7.4076 - val_loss: 4.3084\n",
      "Epoch 3/10\n",
      "934/934 [==============================] - 0s 78us/step - loss: 3.9604 - val_loss: 2.1360\n",
      "Epoch 4/10\n",
      "934/934 [==============================] - 0s 79us/step - loss: 2.0332 - val_loss: 1.0945\n",
      "Epoch 5/10\n",
      "934/934 [==============================] - 0s 77us/step - loss: 1.0646 - val_loss: 0.6000\n",
      "Epoch 6/10\n",
      "934/934 [==============================] - 0s 79us/step - loss: 0.5945 - val_loss: 0.3611\n",
      "Epoch 7/10\n",
      "934/934 [==============================] - 0s 77us/step - loss: 0.3529 - val_loss: 0.2081\n",
      "Epoch 8/10\n",
      "934/934 [==============================] - 0s 78us/step - loss: 0.2125 - val_loss: 0.1302\n",
      "Epoch 9/10\n",
      "934/934 [==============================] - 0s 78us/step - loss: 0.1339 - val_loss: 0.0890\n",
      "Epoch 10/10\n",
      "934/934 [==============================] - 0s 76us/step - loss: 0.0968 - val_loss: 0.0699\n",
      "test_mse : 0.161\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, \n",
    "                epochs=10, \n",
    "                batch_size=10, \n",
    "                verbose=1,\n",
    "                validation_data=(X_val, y_val))\n",
    "\n",
    "test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"test_mse : {:.3f}\".format(test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題6】MNISTをKerasで学習\n",
    "TensorFlowによるMNISTデータセットによる画像の多値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#入力データ（説明変数）を4次元にreshape\n",
    "X_train = X_train.reshape(60000, 28, 28, 1)\n",
    "X_test = X_test.reshape(10000, 28, 28, 1)\n",
    "\n",
    "#正規化\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "#one-hot\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "# y_train = enc.fit_transform(y_train[:, np.newaxis])\n",
    "# y_test = enc.fit_transform(y_test[:, np.newaxis])\n",
    "\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/takahashihideyuki/.pyenv/versions/anaconda3-2019.10/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'conv2d_1/kernel:0' shape=(3, 3, 1, 50) dtype=float32>,\n",
       " <tf.Variable 'conv2d_1/bias:0' shape=(50,) dtype=float32>,\n",
       " <tf.Variable 'conv2d_2/kernel:0' shape=(3, 3, 50, 100) dtype=float32>,\n",
       " <tf.Variable 'conv2d_2/bias:0' shape=(100,) dtype=float32>,\n",
       " <tf.Variable 'dense_1/kernel:0' shape=(2500, 10) dtype=float32>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(50, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(100, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 50)        500       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 50)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 100)       45100     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 100)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2500)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                25010     \n",
      "=================================================================\n",
      "Total params: 70,610\n",
      "Trainable params: 70,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 47s 982us/step - loss: 0.1341 - accuracy: 0.9593 - val_loss: 0.0513 - val_accuracy: 0.9831\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 57s 1ms/step - loss: 0.0478 - accuracy: 0.9850 - val_loss: 0.0485 - val_accuracy: 0.9851\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 56s 1ms/step - loss: 0.0335 - accuracy: 0.9894 - val_loss: 0.0408 - val_accuracy: 0.9875\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 56s 1ms/step - loss: 0.0251 - accuracy: 0.9917 - val_loss: 0.0417 - val_accuracy: 0.9865\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 56s 1ms/step - loss: 0.0182 - accuracy: 0.9941 - val_loss: 0.0369 - val_accuracy: 0.9891\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 56s 1ms/step - loss: 0.0142 - accuracy: 0.9952 - val_loss: 0.0361 - val_accuracy: 0.9893\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 56s 1ms/step - loss: 0.0122 - accuracy: 0.9958 - val_loss: 0.0421 - val_accuracy: 0.9877\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 55s 1ms/step - loss: 0.0097 - accuracy: 0.9969 - val_loss: 0.0421 - val_accuracy: 0.9889\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 54s 1ms/step - loss: 0.0080 - accuracy: 0.9974 - val_loss: 0.0476 - val_accuracy: 0.9885\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 54s 1ms/step - loss: 0.0062 - accuracy: 0.9982 - val_loss: 0.0422 - val_accuracy: 0.9897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fed9843ccd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "                epochs=10, \n",
    "                batch_size=20, \n",
    "                verbose=1,\n",
    "                validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc : 0.990\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題7】（アドバンス課題）PyTorchへの書き換え\n",
    "4種類の問題をPyTorchに書き換えてください。\n",
    "\n",
    "## 回答無し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題8】（アドバンス課題）フレームワークの比較\n",
    "それぞれのフレームワークにはどのような違いがあるかをまとめてください。\n",
    "\n",
    "《視点例》\n",
    "\n",
    "- 計算速度\n",
    "  PyTorchの方がKerasより早い\n",
    "- コードの行数・可読性\n",
    "  Kerasの方がシンプルに記述でき、可読性は高い\n",
    "- 用意されている機能<br>\n",
    "  Pytorch:\n",
    "  transform機能。入力が画像の場合、「リサイズ、画像読み込み、画素を正規化」というような前処理があるが、transform では、この一連の流れを配列の中に列挙することで一括して処理ができる。<br>\n",
    "  Keras:\n",
    "  model.fitで学習可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
