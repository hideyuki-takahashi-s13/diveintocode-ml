{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sprint 深層学習スクラッチ リカレントニューラルネットワーク.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okmWE6Usmwui"
      },
      "source": [
        "# 【問題1】SimpleRNNのフォワードプロパゲーション実装\n",
        "\n",
        "SimpleRNNのクラスSimpleRNNを作成してください。基本構造はFCクラスと同じになります。\n",
        "\n",
        "\n",
        "フォワードプロパゲーションの数式は以下のようになります。ndarrayのshapeがどうなるかを併記しています。\n",
        "\n",
        "\n",
        "バッチサイズをbatch_size、入力の特徴量数をn_features、RNNのノード数をn_nodesとして表記します。活性化関数はtanhとして進めますが、これまでのニューラルネットワーク同様にReLUなどに置き換えられます。\n",
        "\n",
        "$$\n",
        "a_t = x_{t}\\cdot W_{x} + h_{t-1}\\cdot W_{h} + B\\\\\n",
        "h_t = tanh(a_t)\n",
        "$$\n",
        "\n",
        "𝑎𝑡 : 時刻tの活性化関数を通す前の状態 (batch_size, n_nodes)\n",
        "\n",
        "\n",
        "ℎ𝑡 : 時刻tの状態・出力 (batch_size, n_nodes)\n",
        "\n",
        "\n",
        "𝑥𝑡 : 時刻tの入力 (batch_size, n_features)\n",
        "\n",
        "\n",
        "𝑊𝑥 : 入力に対する重み (n_features, n_nodes)\n",
        "\n",
        "\n",
        "ℎ𝑡−1 : 時刻t-1の状態（前の時刻から伝わる順伝播） (batch_size, n_nodes)\n",
        "\n",
        "\n",
        "𝑊ℎ : 状態に対する重み。 (n_nodes, n_nodes)\n",
        "\n",
        "\n",
        "𝐵 : バイアス項 (n_nodes,)\n",
        "\n",
        "\n",
        "初期状態 ℎ0 は全て0とすることが多いですが、任意の値を与えることも可能です。\n",
        "\n",
        "\n",
        "上記の処理を系列数n_sequences回繰り返すことになります。RNN全体への入力 𝑥 は(batch_size, n_sequences, n_features)のような配列で渡されることになり、そこから各時刻の配列を取り出していきます。\n",
        "\n",
        "\n",
        "分類問題であれば、それぞれの時刻のhに対して全結合層とソフトマックス関数（またはシグモイド関数）を使用します。タスクによっては最後の時刻のhだけを使用することもあります。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ycaUDIe59J8"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from copy import deepcopy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Witw1IWe4BlH"
      },
      "source": [
        "class SimpleRNN:\n",
        "    \"\"\"\n",
        "    ノード数n_nodes1からn_nodes2への全結合層\n",
        "    Parameters\n",
        "    ----------\n",
        "    Wx : 次の形のndarray, shape (n_features, n_nodes)\n",
        "    Wh : 次の形のndarray, shape (n_nodes, n_nodes)\n",
        "\n",
        "    optimizer : 最適化手法のインスタンス\n",
        "    \"\"\"\n",
        "    def __init__(self, Wx, Wh, B, optimizer=None, layer=None):\n",
        "        self.optimizer = optimizer\n",
        "        self.layer = layer \n",
        "\n",
        "        self.B = B\n",
        "        self.Wx = Wx\n",
        "        self.Wh = Wh\n",
        "        \n",
        "    def forward(self, X, h_s, A_list=[], cnt = 0):\n",
        "        \"\"\"\n",
        "        フォワード\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (batch_size, n_sequences, n_features)\n",
        "            入力\n",
        "        h_prev : 次の形のndarray, shape (batch_size, n_nodes)\n",
        "                 入力\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        h : 次の形のndarray, shape (batch_size, n_nodes)\n",
        "            出力\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "\n",
        "        batch_size = X.shape[0]\n",
        "        n_sequences = X.shape[1]\n",
        "        n_features = X.shape[2]\n",
        "        n_nodes = self.Wx.shape[1]\n",
        "\n",
        "        A = X[:, cnt, :] @ self.Wx + h_s[:, cnt-1, :] @ self.Wh + self.B\n",
        "        A_list.append(A.tolist())\n",
        "        h_s[:, cnt, :] = np.tanh(A) #ハイパボリックタンジェント関数\n",
        "\n",
        "        self.A = np.array(A_list)\n",
        "\n",
        "        cnt += 1\n",
        "        if cnt == n_sequences:\n",
        "            h = h_s[:, cnt-1, :]\n",
        "            return h\n",
        "        \n",
        "        return self.forward(X, h_s, A_list, cnt = cnt)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8o2AOWc6Xs_"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EggxNDl6bFN"
      },
      "source": [
        "# 【問題2】小さな配列でのフォワードプロパゲーションの実験\n",
        "小さな配列でフォワードプロパゲーションを考えてみます。\n",
        "\n",
        "\n",
        "入力x、初期状態h、重みw_xとw_h、バイアスbを次のようにします。\n",
        "\n",
        "\n",
        "ここで配列xの軸はバッチサイズ、系列数、特徴量数の順番です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VlkJ2xX6XwT"
      },
      "source": [
        "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
        "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
        "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
        "batch_size = x.shape[0] # 1\n",
        "n_sequences = x.shape[1] # 3\n",
        "n_features = x.shape[2] # 2\n",
        "n_nodes = w_x.shape[1] # 4\n",
        "h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
        "h_s = np.zeros((batch_size, n_sequences, n_nodes))\n",
        "b = np.array([1, 1, 1, 1]) # (n_nodes,)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4NJFA8G6kT-"
      },
      "source": [
        "フォワードプロパゲーションの出力が次のようになることを作成したコードで確認してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp8R6spp6hnD"
      },
      "source": [
        "h = np.array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]]) # (batch_size, n_nodes)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw62xGHI6hkC",
        "outputId": "b147eaff-1fe1-4be6-92a4-5be8f9f40464",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#フォワードプロパゲーション\n",
        "SRNN = SimpleRNN(w_x, w_h, b)\n",
        "h = SRNN.forward(x, h_s)\n",
        "print(h)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.79494228 0.81839002 0.83939649 0.85584174]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvPnTlptqSna"
      },
      "source": [
        "### 出力結果が同じになることを確認した。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI48HTc5aauW"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCAnH3Mbtr9Y"
      },
      "source": [
        "# 【問題3】（アドバンス課題）バックプロパゲーションの実装\n",
        "バックプロパゲーションを実装してください。\n",
        "\n",
        "\n",
        "RNNの内部は全結合層を組み合わせた形になっているので、更新式は全結合層などと同様です。\n",
        "\n",
        "$$\n",
        "W_x^{\\prime} = W_x - \\alpha \\frac{\\partial L}{\\partial W_x} \\\\\n",
        "W_h^{\\prime} = W_h - \\alpha \\frac{\\partial L}{\\partial W_h} \\\\\n",
        "B^{\\prime} = B - \\alpha \\frac{\\partial L}{\\partial B}\n",
        "$$\n",
        "\n",
        "𝛼 : 学習率\n",
        "\n",
        "\n",
        "$\\frac{\\partial L}{\\partial W_x}$ : 𝑊𝑥 に関する損失 𝐿 の勾配\n",
        "\n",
        "\n",
        "$\\frac{\\partial L}{\\partial W_h}$ : 𝑊ℎ に関する損失 𝐿 の勾配\n",
        "\n",
        "\n",
        "$\\frac{\\partial L}{\\partial B}$ : 𝐵 に関する損失 𝐿 の勾配\n",
        "\n",
        "\n",
        "勾配を求めるためのバックプロパゲーションの数式が以下です。\n",
        "\n",
        "$\n",
        "\\frac{\\partial h_t}{\\partial a_t} = \\frac{\\partial L}{\\partial h_t} ×(1-tanh^2(a_t))\n",
        "$\n",
        "\n",
        "$\n",
        "\\frac{\\partial L}{\\partial B} = \\frac{\\partial h_t}{\\partial a_t}\n",
        "$\n",
        "\n",
        "$\n",
        "\\frac{\\partial L}{\\partial W_x} = x_{t}^{T}\\cdot \\frac{\\partial h_t}{\\partial a_t}\n",
        "$\n",
        "\n",
        "$\n",
        "\\frac{\\partial L}{\\partial W_h} = h_{t-1}^{T}\\cdot \\frac{\\partial h_t}{\\partial a_t}\n",
        "$\n",
        "\n",
        "＊$\\frac{\\partial L}{\\partial h_t}$ は前の時刻からの状態の誤差と出力の誤差の合計です。hは順伝播時に出力と次の層に伝わる状態双方に使われているからです。\n",
        "\n",
        "前の時刻や層に流す誤差の数式は以下です。\n",
        "\n",
        "$\n",
        "\\frac{\\partial L}{\\partial h_{t-1}} = \\frac{\\partial h_t}{\\partial a_t}\\cdot W_{h}^{T}\n",
        "$\n",
        "\n",
        "$\n",
        "\\frac{\\partial L}{\\partial x_{t}} = \\frac{\\partial h_t}{\\partial a_t}\\cdot W_{x}^{T}\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6ljGym27QHY"
      },
      "source": [
        "### 魚本２を参考に作成（写経）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuHIKI6W5YC9"
      },
      "source": [
        "class RNN:\n",
        "    def __init__(self, Wx, Wh, b):\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        Wx, Wh, b = self.params\n",
        "        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
        "        h_next = np.tanh(t)\n",
        "\n",
        "        self.cache = (x, h_prev, h_next)\n",
        "        return h_next\n",
        "\n",
        "    def backward(self, dh_next):\n",
        "        Wx, Wh, b = self.params\n",
        "        x, h_prev, h_next = self.cache\n",
        "\n",
        "        dt = dh_next * (1 - h_next ** 2)\n",
        "        db = np.sum(dt, axis=0)\n",
        "        dWh = np.dot(h_prev.T, dt)\n",
        "        dh_prev = np.dot(dt, Wh.T)\n",
        "        dWx = np.dot(x.T, dt)\n",
        "        dx = np.dot(dt, Wx.T)\n",
        "\n",
        "        self.grads[0][...] = dWx\n",
        "        self.grads[1][...] = dWh\n",
        "        self.grads[2][...] = db\n",
        "\n",
        "        return dx, dh_prev\n",
        "\n",
        "\n",
        "class TimeRNN:\n",
        "    def __init__(self, Wx, Wh, b, stateful=False):\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.layers = None\n",
        "\n",
        "        self.h, self.dh = None, None\n",
        "        self.stateful = stateful\n",
        "\n",
        "    def forward(self, xs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, D = xs.shape\n",
        "        D, H = Wx.shape\n",
        "\n",
        "        self.layers = []\n",
        "        hs = np.empty((N, T, H), dtype='f')\n",
        "\n",
        "        if not self.stateful or self.h is None:\n",
        "            self.h = np.zeros((N, H), dtype='f')\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = RNN(*self.params)\n",
        "            self.h = layer.forward(xs[:, t, :], self.h)\n",
        "            hs[:, t, :] = self.h\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        return hs\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, H = dhs.shape\n",
        "        D, H = Wx.shape\n",
        "\n",
        "        dxs = np.empty((N, T, D), dtype='f')\n",
        "        dh = 0\n",
        "        grads = [0, 0, 0]\n",
        "        for t in reversed(range(T)):\n",
        "            layer = self.layers[t]\n",
        "            dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
        "            dxs[:, t, :] = dx\n",
        "\n",
        "            for i, grad in enumerate(layer.grads):\n",
        "                grads[i] += grad\n",
        "\n",
        "        for i, grad in enumerate(grads):\n",
        "            self.grads[i][...] = grad\n",
        "        self.dh = dh\n",
        "\n",
        "        return dxs\n",
        "\n",
        "    def set_state(self, h):\n",
        "        self.h = h\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.h = None"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rgQhJPq5fUS",
        "outputId": "3db2508b-cfa3-42a4-c64f-9cafc69903ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "TRNN = TimeRNN(w_x, w_h, b, stateful=False)\n",
        "hs = TRNN.forward(x)\n",
        "hs"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.76188797, 0.76213956, 0.762391  , 0.7625584 ],\n",
              "        [0.792209  , 0.8141834 , 0.8340491 , 0.84977716],\n",
              "        [0.79494226, 0.81839   , 0.8393965 , 0.85584176]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBXxWpT11i7l",
        "outputId": "e76a65dd-de5d-4b3a-986f-e5dc98ce8326",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "y = np.array([1, 1, 1, 1])\n",
        "dhs = y - hs\n",
        "dxs = TRNN.backward(dhs)\n",
        "dxs"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.01684319, 0.02416234],\n",
              "        [0.00868361, 0.01314551],\n",
              "        [0.00762592, 0.01166947]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_JVo4twRFBm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}